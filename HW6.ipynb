{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 (20')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>\n",
    "    \n",
    "Please submit your assignment as an HTML or PDF file.\n",
    "\n",
    "Print your name (First and Last) below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:51:00.503562Z",
     "start_time": "2025-10-05T20:51:00.466325Z"
    }
   },
   "source": [
    "# Write your answer here:\n",
    "Sapna David"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3194835289.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mSapna David\u001B[39m\n          ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>\n",
    "    \n",
    "Import the `pandas`, `matplotlib.pyplot`, `numpy`, `scipy` libraries and assign them with proper nicknames."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:04:19.142978Z",
     "start_time": "2025-10-05T23:04:19.069009Z"
    }
   },
   "source": [
    "# Write your answer here:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import os\n",
    "import openpyxl\n"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1703902.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mpip install pandas openpyxl\u001B[39m\n        ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:49:22.739920Z",
     "start_time": "2025-10-05T20:49:22.718536Z"
    }
   },
   "source": [
    "# Do not delete this code section.\n",
    "np.random.seed(100)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a function that output marginal summary statistics and missing values for continuous variable (10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>\n",
    "\n",
    "**Write the function with `def`** (6')\n",
    "- Given a vector of continuous measure, you are asked to write a function named `fn_marginal_continuous`.\n",
    "- The function has one parameter `input_vec` and outputs a list of summary measure and the number of missing values.\n",
    "- Check the missingness of the `input_vec` using `np.isnan()` or `pd.isna()` functions.\n",
    "    - In our first case, we will assume the `input_vec` is an numpy array with missing values marked with `np.nan`.\n",
    "- The list of summary measure can be either **(mean, std)** or **(median, q1, q3)** depending on the normality assumption.\n",
    "    - To determine the normality assumption, you can rely on the p-value of the Shapiro-Wilk test.\n",
    "    - Relevant functions can be found here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html \n",
    "    - If the p-value < 0.05, it is not normally distributed, otherwise, you can treat it as normally distributed.\n",
    "    - Think about what measure to report based on the normality assumption.\n",
    "    - Part of relevant functions can be found here: https://numpy.org/doc/2.0/reference/generated/numpy.nanmean.html\n",
    "- The return statement should include two components: `missing_num` and `output_ls` (your summmary measure).\n",
    "    - The missing numbers should be of type `int` instead of `np.int64`, and summary values should be of type `float` instead of `np.float64`.\n",
    "    - Round all your summary values such that they have no more than **3** digits after the decimals. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:06:43.459086Z",
     "start_time": "2025-10-05T22:06:43.365012Z"
    }
   },
   "source": [
    "# Write your defined function in this code chunk only.\n",
    "from scipy import stats\n",
    "\n",
    "def fn_marginal_continuous(input_vec):\n",
    "    # Count missing values\n",
    "    missing_num = int(np.sum(np.isnan(input_vec)))\n",
    "\n",
    "    # Remove missing values for calculations\n",
    "    valid_data = input_vec[~np.isnan(input_vec)]\n",
    "\n",
    "    # Conduct Shapiro-Wilk test for normality\n",
    "    stat, p_value = stats.shapiro(valid_data)\n",
    "\n",
    "    # Check if data is normal\n",
    "    if p_value < 0.05:\n",
    "        # Not normally distributed → use median, Q1, Q3\n",
    "        median = float(np.round(np.nanmedian(valid_data), 3))\n",
    "        q1 = float(np.round(np.nanpercentile(valid_data, 25), 3))\n",
    "        q3 = float(np.round(np.nanpercentile(valid_data, 75), 3))\n",
    "        output_ls = [median, q1, q3]\n",
    "    else:\n",
    "        # Normally distributed → use mean, standard deviation\n",
    "        mean = float(np.round(np.nanmean(valid_data), 3))\n",
    "        std = float(np.round(np.nanstd(valid_data), 3))\n",
    "        output_ls = [mean, std]\n",
    "    return missing_num, output_ls\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>\n",
    "\n",
    "**Test your function with the following different arguments:** (4') <br>\n",
    "For each scenario, please export the results as `missing_num_x`, `output_ls_x` and print them out separately.\n",
    "1. A standard normal random vector with a sample size of 100, named `input_vec_1`.\n",
    "2. A Chi-squared random vector with a degree of freedom 1 and a sample size of 100, named `input_vec_2`.\n",
    "3. Change the first element of `input_vec_1` as `np.nan` and create a new array named `input_vec_3`.\n",
    "    - Note that to create a copy of an numpy array, use `np.copy()` first.\n",
    "4. Change the last element of `input_vec_2` as `np.nan` and create a new array named `input_vec_4`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:06:47.938709Z",
     "start_time": "2025-10-05T22:06:47.843364Z"
    }
   },
   "source": [
    "# Write your own code for scenario 1:\n",
    "input_vec_1 = np.random.normal(0, 1, 100)\n",
    "\n",
    "missing_num_1, output_ls_1 = fn_marginal_continuous(input_vec_1)\n",
    "\n",
    "print(\"Scenario 1:\")\n",
    "print(\"missing_num_1:\", missing_num_1)\n",
    "print(\"output_ls_1:\", output_ls_1)\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1:\n",
      "missing_num_1: 0\n",
      "output_ls_1: [-0.117, 1.06]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:10:29.362645Z",
     "start_time": "2025-10-05T22:10:29.292470Z"
    }
   },
   "source": [
    "# Write your own code for scenario 2:\n",
    "input_vec_2 = np.random.chisquare(1, 100)\n",
    "\n",
    "missing_num_2, output_ls_2 = fn_marginal_continuous(input_vec_2)\n",
    "\n",
    "print(\"Scenario 2:\")\n",
    "print(\"missing_num_2:\", missing_num_2)\n",
    "print(\"output_ls_2:\", output_ls_2)\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 2:\n",
      "missing_num_2: 0\n",
      "output_ls_2: [0.285, 0.068, 1.285]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:51:17.918983Z",
     "start_time": "2025-10-05T20:51:17.884155Z"
    }
   },
   "source": [
    "# Write your own code for scenario 3:\n",
    "input_vec_3 = np.copy(input_vec_1)\n",
    "input_vec_3[0] = np.nan\n",
    "\n",
    "missing_num_3, output_ls_3 = fn_marginal_continuous(input_vec_3)\n",
    "\n",
    "print(\"Scenario 3:\")\n",
    "print(\"missing_num_3:\", missing_num_3)\n",
    "print(\"output_ls_3:\", output_ls_3)\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 3:\n",
      "missing_num_3: 1\n",
      "output_ls_3: [0.206, 1.078]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:51:19.587158Z",
     "start_time": "2025-10-05T20:51:19.573538Z"
    }
   },
   "source": [
    "# Write your own code for scenario 4:\n",
    "input_vec_4 = np.copy(input_vec_2)\n",
    "input_vec_4[-1] = np.nan\n",
    "\n",
    "missing_num_4, output_ls_4 = fn_marginal_continuous(input_vec_4)\n",
    "\n",
    "print(\"Scenario 4:\")\n",
    "print(\"missing_num_4:\", missing_num_4)\n",
    "print(\"output_ls_4:\", output_ls_4)\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 4:\n",
      "missing_num_4: 1\n",
      "output_ls_4: [0.505, 0.182, 1.269]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a function that output marginal summary statistics and missing values for categorical variable (5')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "**Write the function with `def`**\n",
    "- Given a column vector, you are asked to write a function named `fn_marginal_categorical`.\n",
    "- The function has one parameter named `input_vec` and outputs a list of summary measure and the number of missing values.\n",
    "- Check the missingness of the `input_vec` using `np.isnan()` or `pd.isna()` functions.\n",
    "    - In our second case, we will assume the `input_vec` is a column from a pandas DataFrame with missing values marked with `np.nan`.\n",
    "    - You can use both functions to identify missing values and yield the number of missingness.\n",
    "    - Use `pd.Series.value_counts()` function to obtain the frequency and proportion of `input_vec`, denoted as `tab_count` and `tab_percent`, respectively.\n",
    "    - Details can be found here: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n",
    "    - For proportion, please use percentage (0-100%). You can ignore % when reporting.\n",
    "- The return statement should include two components: `missing_num` and `output_tab` (your summmary measure).\n",
    "    - For your `output_tab`, combine the count and proportion together using `pd.concat()`. Your `output_tab` should have three columns: variable name, count, and proportion.\n",
    "    - Details can be found here: https://pandas.pydata.org/docs/reference/api/pandas.concat.html\n",
    "    - The missing numbers should be of type `int` instead of `np.int64`, and percentgae values should be of type `float` instead of `np.float64`.\n",
    "    - Round all your relevant summary values such that they have no more than **2** digits after the decimals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T22:06:54.826448Z",
     "start_time": "2025-10-05T22:06:54.803071Z"
    }
   },
   "source": [
    "# Write your defined function in this code chunk only.\n",
    "def fn_marginal_categorical(input_vec):\n",
    "    # Count missing values\n",
    "    missing_num = int(pd.isna(input_vec).sum())\n",
    "\n",
    "    # Remove missing values for tabulation\n",
    "    valid_data = input_vec.dropna()\n",
    "\n",
    "    # Frequency count\n",
    "    tab_count = valid_data.value_counts(dropna=False)\n",
    "\n",
    "    # Proportion in percentage\n",
    "    tab_percent = valid_data.value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "    # Round proportions and convert to float\n",
    "    tab_percent = tab_percent.round(2).astype(float)\n",
    "\n",
    "    # Combine counts and percentages into a single DataFrame\n",
    "    output_tab = pd.concat([tab_count, tab_percent], axis=1)\n",
    "    output_tab.columns = ['count', 'proportion']\n",
    "\n",
    "    # Reset index and rename first column as 'variable'\n",
    "    output_tab = output_tab.reset_index()\n",
    "    output_tab.columns = ['variable', 'count', 'proportion']\n",
    "\n",
    "    return missing_num, output_tab"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test your written functions with a real dataset. (3')\n",
    "\n",
    "<font size='4'>\n",
    "    \n",
    "**Test the summary function for the continuous measure** (1')\n",
    "- Load the `PTSD dataset.xlsx` and name it as `ptsd_df`.\n",
    "    - The dataset should be stored under `data` folder when you sync changes and fetch origins the GitHub repository.\n",
    "- Use the column `pcl5month_score.baseline` as the input vector. This is a continuous measure.\n",
    "- Extract the corresponding column and give it a name `pcl5month_base`.\n",
    "- (*Optional*) You convert from a pandas.dataframe to an numpy array."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:29:26.573364Z",
     "start_time": "2025-10-05T23:29:23.447286Z"
    }
   },
   "source": [
    "# import ptsd dataset in this code section only (no point for this part as you have done it a few times)\n",
    "os.chdir(\"/Users/sapnadavid/Documents/GitHub/BIOS-584/BIOS-584\")\n",
    "print( os.getcwd())\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/Users/sapnadavid/Documents/GitHub/BIOS-584/BIOS-584/data/PTSD dataset.xlsx\"\n",
    "\n",
    "# Load the specific sheet\n",
    "ptsd_df = pd.read_excel(file_path, sheet_name=\"main_dataset\", engine=\"openpyxl\")\n",
    "\n",
    "# Check that it loaded properly\n",
    "print(\"Columns in 'main_dataset':\\n\", ptsd_df.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sapnadavid/Documents/GitHub/BIOS-584/BIOS-584\n",
      "Columns in 'main_dataset':\n",
      " ['record_id', 'ptsdpresent_caps', 'caps_minuspcl', 'caps_minuspcl_code', 'caps_intake', 'pcl5_score_intake', 'pcl5month_score.baseline', 'pcl5week_score.completion', 'pcl5month_score.3_month_follow_up', 'mdd_code', 'ctq_total_score', 'sexual_trauma', 'sud_code', 'phq9_score_intake', 'phq9_score.baseline', 'age_iop', 'gender_code', 'race_code', 'ethnicity_code', 'sexualorient_code', 'education_code', 'employment_code', 'rank_code', 'branch_code', 'total_ptsd_sx_caps', 'caps_1', 'caps_1_sev', 'caps_2', 'caps_2_sev', 'caps_3', 'caps_3_sev', 'caps_4', 'caps_4_sev', 'caps_5', 'caps_5_sev', 'b_sx_caps', 'b_sev_caps', 'criterion_b', 'caps_6', 'caps_6_sev', 'caps_7', 'caps_7_sev', 'c_sx_caps', 'c_sev_caps', 'criterion_c', 'caps_8', 'caps_8_sev', 'caps_9', 'caps_9_sev', 'caps_10', 'caps_10_sev', 'caps_11', 'caps_11_sev', 'caps_12', 'caps_12_sev', 'caps_13', 'caps_13_sev', 'caps_14', 'caps_14_sev', 'd_sx_caps', 'd_sev_caps', 'criterion_d', 'caps_15', 'caps_15_sev', 'caps_16', 'caps_16_sev', 'caps_17', 'caps_17_sev', 'caps_18', 'caps_18_sev', 'caps_19', 'caps_19_sev', 'caps_20', 'caps_20_sev', 'e_sx_caps', 'e_sev_caps', 'criterion_e', 'pcl5_1_intake', 'pcl5_2_intake', 'pcl5_3_intake', 'pcl5_4_intake', 'pcl5_5_intake', 'pcl5_6_intake', 'pcl5_7_intake', 'pcl5_8_intake', 'pcl5_9_intake', 'pcl5_10_intake', 'pcl5_11_intake', 'pcl5_12_intake', 'pcl5_13_intake', 'pcl5_14_intake', 'pcl5_15_intake', 'pcl5_16_intake', 'pcl5_17_intake', 'pcl5_18_intake', 'pcl5_19_intake', 'pcl5_20_intake', 'pcl_5_complete_intake', 'diagnosisa_caps_dissociative_sx', 'diagnosisb_caps_21_21_delayed_onset', 'caps_22_criterion_f', 'caps_23', 'caps_23_sev', 'caps_24', 'caps_24_sev', 'caps_25', 'caps_25_sev', 'g_sx_caps', 'g_sev_caps', 'criterion_g', 'caps_26', 'caps_27', 'caps_28', 'caps_29', 'caps_29_sev', 'caps_30', 'caps_30_sev', 'capscomplete', 'caps_method', 'caps_complete', 'PresentingComplaint', 'TargetTrauma', 'HistoryofPsychiatricDiagnoses', 'Currentpsychiatrictreatment', 'Pastpsychiatrictreatment', 'CriteriaA', 'Trauma1LEC', 'Trauma1Military', 'Trauma1ExpWit', 'Trauma1Suicide', 'Trauma2LEC', 'Trauma2Military', 'Trauma2ExpWit', 'Trauma2Suicide', 'Trauma3LEC', 'Trauma3Military', 'Trauma3ExpWit', 'Trauma3Suicide', 'TraumaNotes', 'pcl5_1w.completion', 'pcl5_2w.completion', 'pcl5_3w.completion', 'pcl5_4w.completion', 'pcl5_5w.completion', 'pcl5_6w.completion', 'pcl5_7w.completion', 'pcl5_8w.completion', 'pcl5_9w.completion', 'pcl5_10w.completion', 'pcl5_11w.completion', 'pcl5_12w.completion', 'pcl5_13w.completion', 'pcl5_14w.completion', 'pcl5_15w.completion', 'pcl5_16w.completion', 'pcl5_17w.completion', 'pcl5_18w.completion', 'pcl5_19w.completion', 'pcl5_20w.completion', 'pcl5w_complete', 'pcl5_1.12_month_follow_up', 'pcl5_2.12_month_follow_up', 'pcl5_3.12_month_follow_up', 'pcl5_4.12_month_follow_up', 'pcl5_5.12_month_follow_up', 'pcl5_6.12_month_follow_up', 'pcl5_7.12_month_follow_up', 'pcl5_8.12_month_follow_up', 'pcl5_9.12_month_follow_up', 'pcl5_10.12_month_follow_up', 'pcl5_11.12_month_follow_up', 'pcl5_12.12_month_follow_up', 'pcl5_13.12_month_follow_up', 'pcl5_14.12_month_follow_up', 'pcl5_15.12_month_follow_up', 'pcl5_16.12_month_follow_up', 'pcl5_17.12_month_follow_up', 'pcl5_18.12_month_follow_up', 'pcl5_19.12_month_follow_up', 'pcl5_20.12_month_follow_up', 'pcl_5_complete.12_month_follow_up', 'phq_date.12_month_follow_up', 'phq_1.12_month_follow_up', 'phq_2.12_month_follow_up', 'phq_3.12_month_follow_up', 'phq_4.12_month_follow_up', 'phq_5.12_month_follow_up', 'phq_6.12_month_follow_up', 'phq_7.12_month_follow_up', 'phq_8.12_month_follow_up', 'phq_9.12_month_follow_up', 'phq_10.12_month_follow_up', 'phq_9_complete.12_month_follow_up', 'phq9_score.12_month_follow_up', 'pcl5month_score.12_month_follow_up', 'pcl5_1.3_month_follow_up', 'pcl5_2.3_month_follow_up', 'pcl5_3.3_month_follow_up', 'pcl5_4.3_month_follow_up', 'pcl5_5.3_month_follow_up', 'pcl5_6.3_month_follow_up', 'pcl5_7.3_month_follow_up', 'pcl5_8.3_month_follow_up', 'pcl5_9.3_month_follow_up', 'pcl5_10.3_month_follow_up', 'pcl5_11.3_month_follow_up', 'pcl5_12.3_month_follow_up', 'pcl5_13.3_month_follow_up', 'pcl5_14.3_month_follow_up', 'pcl5_15.3_month_follow_up', 'pcl5_16.3_month_follow_up', 'pcl5_17.3_month_follow_up', 'pcl5_18.3_month_follow_up', 'pcl5_19.3_month_follow_up', 'pcl5_20.3_month_follow_up', 'pcl_5_complete.3_month_follow_up', 'phq_date.3_month_follow_up', 'phq_1.3_month_follow_up', 'phq_2.3_month_follow_up', 'phq_3.3_month_follow_up', 'phq_4.3_month_follow_up', 'phq_5.3_month_follow_up', 'phq_6.3_month_follow_up', 'phq_7.3_month_follow_up', 'phq_8.3_month_follow_up', 'phq_9.3_month_follow_up', 'phq_10.3_month_follow_up', 'phq_9_complete.3_month_follow_up', 'phq9_score.3_month_follow_up', 'pcl5_1.6_month_follow_up', 'pcl5_2.6_month_follow_up', 'pcl5_3.6_month_follow_up', 'pcl5_4.6_month_follow_up', 'pcl5_5.6_month_follow_up', 'pcl5_6.6_month_follow_up', 'pcl5_7.6_month_follow_up', 'pcl5_8.6_month_follow_up', 'pcl5_9.6_month_follow_up', 'pcl5_10.6_month_follow_up', 'pcl5_11.6_month_follow_up', 'pcl5_12.6_month_follow_up', 'pcl5_13.6_month_follow_up', 'pcl5_14.6_month_follow_up', 'pcl5_15.6_month_follow_up', 'pcl5_16.6_month_follow_up', 'pcl5_17.6_month_follow_up', 'pcl5_18.6_month_follow_up', 'pcl5_19.6_month_follow_up', 'pcl5_20.6_month_follow_up', 'pcl_5_complete.6_month_follow_up', 'phq_date.6_month_follow_up', 'phq_1.6_month_follow_up', 'phq_2.6_month_follow_up', 'phq_3.6_month_follow_up', 'phq_4.6_month_follow_up', 'phq_5.6_month_follow_up', 'phq_6.6_month_follow_up', 'phq_7.6_month_follow_up', 'phq_8.6_month_follow_up', 'phq_9.6_month_follow_up', 'phq_10.6_month_follow_up', 'phq_9_complete.6_month_follow_up', 'phq9_score.6_month_follow_up', 'pcl5month_score.6_month_follow_up', 'pcl5_1.baseline', 'pcl5_2.baseline', 'pcl5_3.baseline', 'pcl5_4.baseline', 'pcl5_5.baseline', 'pcl5_6.baseline', 'pcl5_7.baseline', 'pcl5_8.baseline', 'pcl5_9.baseline', 'pcl5_10.baseline', 'pcl5_11.baseline', 'pcl5_12.baseline', 'pcl5_13.baseline', 'pcl5_14.baseline', 'pcl5_15.baseline', 'pcl5_16.baseline', 'pcl5_17.baseline', 'pcl5_18.baseline', 'pcl5_19.baseline', 'pcl5_20.baseline', 'pcl_5_complete.baseline', 'phq_date.baseline', 'phq_1.baseline', 'phq_2.baseline', 'phq_3.baseline', 'phq_4.baseline', 'phq_5.baseline', 'phq_6.baseline', 'phq_7.baseline', 'phq_8.baseline', 'phq_9.baseline', 'phq_10.baseline', 'phq_9_complete.baseline', 'pcl5_1.completion', 'pcl5_2.completion', 'pcl5_3.completion', 'pcl5_4.completion', 'pcl5_5.completion', 'pcl5_6.completion', 'pcl5_7.completion', 'pcl5_8.completion', 'pcl5_9.completion', 'pcl5_10.completion', 'pcl5_11.completion', 'pcl5_12.completion', 'pcl5_13.completion', 'pcl5_14.completion', 'pcl5_15.completion', 'pcl5_16.completion', 'pcl5_17.completion', 'pcl5_18.completion', 'pcl5_19.completion', 'pcl5_20.completion', 'pcl_5_complete.completion', 'phq_date.completion', 'phq_1.completion', 'phq_2.completion', 'phq_3.completion', 'phq_4.completion', 'phq_5.completion', 'phq_6.completion', 'phq_7.completion', 'phq_8.completion', 'phq_9.completion', 'phq_10.completion', 'phq_9_complete.completion', 'phq9_score.completion', 'pcl5month_score.completion', 'height', 'weight', 'bmi', 'primary_therapy6', 'iop_track', 'tbi_patient', 'care_type', 'telehealth_location', 'rtms_patient', 'tdcs_patient', 'icf', 'adl_2_complete', 'childtra_1', 'childtra_2', 'childtra_3', 'childtra_4', 'childtra_5', 'childtra_6', 'childtra_7', 'childtra_8', 'childtra_9', 'childtra_10', 'childtra_11', 'childtra_12', 'childtra_13', 'childtra_14', 'childtra_15', 'childtra_16', 'childtra_17', 'childtra_18', 'childtra_19', 'childtra_20', 'childtra_21', 'childtra_22', 'childtra_23', 'childtra_24', 'childtra_25', 'childtra_26', 'childtra_27', 'childtra_28', 'ctq_complete', 'phq_date_intake', 'phq_1_intake', 'phq_2_intake', 'phq_3_intake', 'phq_4_intake', 'phq_5_intake', 'phq_6_intake', 'phq_7_intake', 'phq_8_intake', 'phq_9_intake', 'phq_10_intake', 'phq_9_complete_intake', 'ctq_emo_score', 'ctq_phys_score', 'ctq_sexabuse_score', 'ctq_emoneglect_score', 'ctq_physneglect_score', 'ctq_emot_cat', 'ctq_phys_cat', 'ctq_sex_cat', 'ctq_emot_neg_cat', 'ctq_phys_neg_cat', 'survey_date2', 'survey_date4', 'survey_date7', 'survey_date9', 'pcl5_score2', 'pcl5_score4', 'pcl5_score7', 'pcl5_score9', 'phq9_score2', 'phq9_score4', 'phq9_score7', 'phq9_score9', 'primary_diagnosis_icd', 'secondary_icd_dx_code_1', 'secondary_icd_dx_code_2', 'secondary_icd_dx_code_3', 'secondary_icd_dx_code_4', 'secondary_icd_dx_code_5', 'secondary_icd_dx_code_6', 'secondary_icd_dx_code_7', 'secondary_icd_dx_code_8', 'secondary_icd_dx_code_9', 'secondary_icd_dx_code_10', 'fullcaps', 'gender_src', 'sexual_orientation_src', 'employment_status', 'birth_year', 'marital_status', 'number_of_children', 'branch_src', 'military_service_status_src', 'rank_pay_grade', 'discharge_status', 'state_code', 'va_benefits_indicator', 'wwp_alumni_indicator', 'army', 'airforce', 'marines', 'navy', 'coastguard', 'nationalguard', 'reserve']\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:29:52.438497Z",
     "start_time": "2025-10-05T23:29:52.397426Z"
    }
   },
   "source": [
    "# test continuous measure\n",
    "pcl5month_base = ptsd_df[\"pcl5month_score.baseline\"]\n",
    "\n",
    "# Optional: Convert to a numpy array\n",
    "pcl5month_base_array = np.array(pcl5month_base)\n",
    "\n",
    "# Display basic info\n",
    "print(\"PCL-5 Month Baseline Summary:\")\n",
    "print(pcl5month_base.describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCL-5 Month Baseline Summary:\n",
      "count    474.000000\n",
      "mean      51.164557\n",
      "std       14.544323\n",
      "min        4.000000\n",
      "25%       42.000000\n",
      "50%       51.000000\n",
      "75%       62.000000\n",
      "max       80.000000\n",
      "Name: pcl5month_score.baseline, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>\n",
    "\n",
    "**Test the summary function for the categorical measure** (2')\n",
    "\n",
    "- Use the column `mdd_code` as the input vector. This is a binary vector.\n",
    "1. Extract the corresponding column and give it a name `mdd_code_vec`. Output your results as `missing_num_1` and `tab_1`. Print each element out (You will write `print()` twice).\n",
    "2. Create a copy of `mdd_code_vec` and name it as `mdd_code_vec_2`. Change its first element to `NaN`.\n",
    "    - Rerun the `fn_marginal_categorical()` with the new vector. Output your results as `missing_num_2` and `tab_2`. Print each element out (You will write `print()` twice)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:32:57.395438Z",
     "start_time": "2025-10-05T23:32:57.178410Z"
    }
   },
   "source": [
    "# Write your own code for scenario 1 only:\n",
    "mdd_code_vec = ptsd_df[\"mdd_code\"]\n",
    "\n",
    "missing_num_1, tab_1 = fn_marginal_categorical(mdd_code_vec)\n",
    "\n",
    "\n",
    "print(missing_num_1)\n",
    "print(tab_1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   variable  count  proportion\n",
      "0         0    340       70.39\n",
      "1         1    143       29.61\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:33:11.254190Z",
     "start_time": "2025-10-05T23:33:11.211824Z"
    }
   },
   "source": [
    "# Write your own code for scenario 2 only:\n",
    "mdd_code_vec_2 = mdd_code_vec.copy()\n",
    "mdd_code_vec_2.iloc[0] = np.nan\n",
    "\n",
    "\n",
    "missing_num_2, tab_2 = fn_marginal_categorical(mdd_code_vec_2)\n",
    "\n",
    "\n",
    "print(missing_num_2)\n",
    "print(tab_2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "   variable  count  proportion\n",
      "0       0.0    339       70.33\n",
      "1       1.0    143       29.67\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lambda functions and for loop (2')\n",
    "\n",
    "<font size='4'>\n",
    "\n",
    "- Create a ``` lambda ``` function that checks if column `pcl5_score_intake` is greater than 30, denoted as `fn_pcl5_ptsd_check`.\n",
    "- Create a new list `pcl5_score_intake_ls` that shows `True` if `pcl5_score_intake`>30 and `False` otherwise using a for loop.\n",
    "    - You can also try `map()` function to iterate the function over `pcl5_score_intake`.\n",
    "    - Syntax is simple: `map(function_name, iterable, ...)`. \n",
    "- Print out the number of patients with `pcl5_score_intake` over 30 and mark them as **Clinically Significant for PTSD**.\n",
    "    - Your output can be something like `XXX out of YYY patients (ZZZ%) were marked as clinically significant for PTSD.`\n",
    "    - Round your percentage with no more than **2** decimals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T23:34:29.184360Z",
     "start_time": "2025-10-05T23:34:29.054851Z"
    }
   },
   "source": [
    "# Write your own code\n",
    "fn_pcl5_ptsd_check = lambda x: x > 30\n",
    "\n",
    "pcl5_score_intake = ptsd_df[\"pcl5_score_intake\"]\n",
    "\n",
    "# Initialize empty list\n",
    "pcl5_score_intake_ls = []\n",
    "\n",
    "# Loop through each value and apply the lambda function\n",
    "for score in pcl5_score_intake:\n",
    "    pcl5_score_intake_ls.append(fn_pcl5_ptsd_check(score))\n",
    "\n",
    "# check first few elements\n",
    "print(pcl5_score_intake_ls[:10])\n",
    "\n",
    "pcl5_score_intake_ls_map = list(map(fn_pcl5_ptsd_check, pcl5_score_intake))\n",
    "\n",
    "num_clinically_significant = sum(pcl5_score_intake_ls)  # True counts as 1\n",
    "total_patients = len(pcl5_score_intake_ls)\n",
    "\n",
    "# Percentage\n",
    "percent_clinically_significant = round((num_clinically_significant / total_patients) * 100, 2)\n",
    "\n",
    "# Print output\n",
    "print(f\"{num_clinically_significant} out of {total_patients} patients ({percent_clinically_significant}%) were marked as clinically significant for PTSD.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True]\n",
      "451 out of 483 patients (93.37%) were marked as clinically significant for PTSD.\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "45fc1f684f6f416f40889115beff3ddf69879b64cf4bfee48cb72a61e9d15d1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
